{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a385e9d6",
   "metadata": {},
   "source": [
    "# Visualize AFOG Attack on Swin-L\n",
    "This notebook provides tools for visualizing an AFOG attack on a single image for Swin-L. Please following setup instructions in README.md prior to running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()[:os.getcwd().find(\"demos\")])\n",
    "import torch, json\n",
    "import numpy as np\n",
    "\n",
    "from scripts.attack_dino import build_model_main\n",
    "from utils.dino_utils.slconfig import SLConfig\n",
    "from utils.dataset_utils.coco import build_dataset\n",
    "from utils.dino_utils.visualizer import COCOVisualizer\n",
    "from utils.dino_utils import box_ops\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5198686",
   "metadata": {},
   "source": [
    "Choose the locations for the Swin checkpoint and config path. NOTE: change model_checkpoint_path to the filename on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744327a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_path = \"../utils/dino_utils/config/DINO/DINO_4scale_swin.py\"\n",
    "model_checkpoint_path = \"../model_files/checkpoint0011_4scale_swin.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c11a3d",
   "metadata": {},
   "source": [
    "Load in model and prepare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fa0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SLConfig.fromfile(model_config_path) \n",
    "args.device = 'cuda' \n",
    "model, criterion, postprocessors = build_model_main(args)\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
    "model.criterion = criterion\n",
    "model.postprocessors = postprocessors\n",
    "_ = model.eval()\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.to(torch.device('cuda'))\n",
    "# load coco names\n",
    "with open('../utils/dino_utils/coco_id2name.json') as f:\n",
    "    id2name = json.load(f)\n",
    "    id2name = {int(k):v for k,v in id2name.items()}\n",
    "    \n",
    "args.dataset_file = 'coco'\n",
    "args.coco_path = \"../datasets/coco/\" # the path of coco\n",
    "args.fix_size = False\n",
    "\n",
    "dataset_val = build_dataset(image_set='val', args=args)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510c7d2",
   "metadata": {},
   "source": [
    "Helper functions for preparing and attacking a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ground-truth labels\n",
    "def viz_gt(image, targets):\n",
    "    box_label = [id2name[int(item)] for item in targets['labels']]\n",
    "    gt_dict = {\n",
    "        'boxes': targets['boxes'],\n",
    "        'image_id': targets['image_id'],\n",
    "        'size': targets['size'],\n",
    "        'box_label': box_label,\n",
    "    }\n",
    "    vslzr = COCOVisualizer()\n",
    "    vslzr.visualize(image, gt_dict, savedir=None, caption=\"Ground Truth\")\n",
    "\n",
    "# Visualize model predictions\n",
    "def viz_preds(model, image, targets, caption):\n",
    "    output = model(image[None].cuda())\n",
    "    output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
    "\n",
    "    threshold = 0.5 # set a threshold\n",
    "\n",
    "    scores = output['scores']\n",
    "    labels = output['labels']\n",
    "    boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
    "    select_mask = scores > threshold\n",
    "    box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
    "    pred_dict = {\n",
    "        'boxes': boxes[select_mask],\n",
    "        'size': targets['size'],\n",
    "        'box_label': box_label\n",
    "    }\n",
    "    vslzr = COCOVisualizer()\n",
    "    vslzr.visualize(image, pred_dict, savedir=None, caption=caption)\n",
    "\n",
    "from matplotlib.cm import coolwarm\n",
    "def visualize_attack(model, image, targets, attack, mode, n_iter=5):\n",
    "    x_adv, etas, eta_grads, map_grads, maps = attack(model, image.unsqueeze(0).float(), vis=True, n_iter = n_iter, mode=\"baseline\")\n",
    "    \n",
    "    print([(np.max(m), np.min(m)) for m in maps])\n",
    "    \n",
    "    # normalize grads to make them easier to see\n",
    "    perts = np.multiply(etas, maps)\n",
    "    perts_norm = [(grad - np.min(grad)) / (np.max(grad) - np.min(grad)) for grad in perts]\n",
    "    perts_norm = [np.clip(1 - np.abs(pert - np.median(pert)), 0, 1) for pert in perts_norm]\n",
    "    \n",
    "    eta_grads_norm = [(grad - np.min(grad)) / (np.max(grad)-np.min(grad)) for grad in eta_grads]\n",
    "    eta_grads_norm = [np.clip(1 - 5*np.abs(grad - np.median(grad)), 0, 1) for grad in eta_grads_norm]\n",
    "    \n",
    "    map_grads_norm = [(grad - np.min(grad)) / (0.001 + np.max(grad)-np.min(grad)) for grad in map_grads]\n",
    "    map_grads_norm = [np.clip(1 - 5*np.abs(grad - np.median(grad)), 0, 1) for grad in map_grads_norm]\n",
    "    \n",
    "    etas_norm = [(eta - np.min(eta)) / (np.max(eta)-np.min(eta)) for eta in etas]\n",
    "    etas_norm = [100 * np.clip(1 - np.abs(eta - np.median(eta)), 0, 1) for eta in etas_norm]\n",
    "    \n",
    "    maps_norm = [(m - np.min(m)) / (0.001 + np.max(m)-np.min(m)) for m in maps]\n",
    "    maps_norm = [np.clip(1 - np.abs(m - np.median(m)), 0, 1) for m in maps_norm]\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=n_iter, ncols=5, figsize=(15, n_iter*3+8))\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        colormap = \"RdYlBu\"\n",
    "        ax[i][0].matshow(perts_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "        ax[i][1].matshow(eta_grads_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "        ax[i][2].matshow(maps_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "        ax[i][3].matshow(map_grads_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "        ax[i][4].matshow(etas_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "\n",
    "        ax[i][0].set_title(\"After Iter \" + str(i+1) + \": A*P\")\n",
    "        ax[i][1].set_title(\"P Grad\")\n",
    "        ax[i][2].set_title(\"A Map\")\n",
    "        ax[i][3].set_title(\"A Grad\")\n",
    "        ax[i][4].set_title(\"P\")\n",
    "    plt.show()\n",
    "    print(etas[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007a368",
   "metadata": {},
   "source": [
    "Select an image from the dataset. Display the ground truth labels and the benign predictions from Swin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4169f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_NUM=500\n",
    "\n",
    "image, targets = dataset_val[IM_NUM]\n",
    "print(\"--- Image ID:\", targets[\"image_id\"][0].item(), \"---\")\n",
    "viz_gt(image, targets)\n",
    "viz_preds(model, image, targets, caption=\"Prediction (Benign)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63751c81",
   "metadata": {},
   "source": [
    "Attack the image with AFOG and display the adversarial predictions. Attack mode can be changed by setting mode to either \"baseline\", \"vanishing\", or \"fabrication\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf74024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from afog.attacks import *\n",
    "image_unt = afog(model, image.unsqueeze(0).float(), n_iter=10, mode=\"baseline\")\n",
    "\n",
    "viz_preds(model, image_unt.squeeze().float(), targets, caption=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c53f4",
   "metadata": {},
   "source": [
    "Visualize the internals of AFOG as it iterates, including perturbation map, attention map, and their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccadcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image, targets = dataset_val[0]\n",
    "visualize_attack(model, image, targets, afog, \"baseline\", n_iter=10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e1b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AFOG",
   "language": "python",
   "name": "afog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
