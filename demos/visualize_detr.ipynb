{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d731ceb",
   "metadata": {},
   "source": [
    "# Visualize AFOG Attack on DETR\n",
    "This notebook provides functions for visualizing DETR and its self-attention weights before and after attacking a single COCO image. Please following setup instructions in README.md prior to running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd()[:os.getcwd().find(\"demos\")])\n",
    "import torch, json\n",
    "import numpy as np\n",
    "\n",
    "from utils.detr_utils import build_model\n",
    "from utils.dino_utils.slconfig import SLConfig\n",
    "from utils.dataset_utils.coco import build_dataset\n",
    "from utils.dino_utils.visualizer import COCOVisualizer\n",
    "from utils.dino_utils import box_ops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518a7a1",
   "metadata": {},
   "source": [
    "Helper functions for preparing and displaying one image are in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_attention.ipynb#scrollTo=eg4RK8JiYTEl\n",
    "import torchvision.transforms as T\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32).cuda()\n",
    "    return b\n",
    "\n",
    "\n",
    "def visualize_decoder_weights(model, img, mode=\"single\", caption=\"Image\"):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "    ]\n",
    "    COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "    # propagate through the model\n",
    "    outputs = model(img.unsqueeze(0).cuda())\n",
    "\n",
    "    # keep only predictions with 0.7+ confidence\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > 0.3\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    print(\"Size is\", img.shape)\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], (img.shape[2], img.shape[1]))\n",
    "    \n",
    "    # use lists to store the outputs via up-values\n",
    "    conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
    "\n",
    "    hooks = [\n",
    "        model.backbone[-2].register_forward_hook(\n",
    "            lambda self, inp, output: conv_features.append(output)\n",
    "        ),\n",
    "        model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "            lambda self, inp, output: enc_attn_weights.append(output[1])\n",
    "        ),\n",
    "        model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "            lambda self, inp, output: dec_attn_weights.append(output[1])\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # propagate through the model\n",
    "    outputs = model(img.unsqueeze(0).cuda())\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # don't need the list anymore\n",
    "    conv_features = conv_features[0]\n",
    "    enc_attn_weights = enc_attn_weights[0]\n",
    "    dec_attn_weights = dec_attn_weights[0]\n",
    "    \n",
    "    # get the feature map shape\n",
    "    h, w = conv_features['0'].tensors.shape[-2:]\n",
    "\n",
    "    print(\"There are\", len(bboxes_scaled), \"objects in the figure\")\n",
    "    img = img.permute(1, 2, 0)\n",
    "    img = (img - torch.min(img)) / (torch.max(img) - torch.min(img))\n",
    "    plt.clf()\n",
    "    \n",
    "    if mode == \"multi\":\n",
    "        fig, ax = plt.subplots(ncols=2, nrows=len(bboxes_scaled), figsize=(2*5, len(bboxes_scaled) * 5))\n",
    "        colors = COLORS * 100\n",
    "        i = 0\n",
    "\n",
    "        for idx, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), bboxes_scaled.detach().cpu().numpy()):\n",
    "            if len(bboxes_scaled) > 1:\n",
    "        #         ax = ax_i[0]\n",
    "                ax[i,0].imshow(dec_attn_weights[0, idx].view(h, w).detach().cpu().numpy())\n",
    "                ax[i,0].axis('off')\n",
    "                ax[i,0].set_title(f'query id: {idx.item()}')\n",
    "                #ax = ax_i[1]\n",
    "                ax[i,1].imshow(img)\n",
    "                ax[i,1].add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                           fill=False, color='blue', linewidth=3))\n",
    "                ax[i,1].axis('off')\n",
    "                ax[i,1].set_title(CLASSES[probas[idx].argmax()])\n",
    "                i += 1\n",
    "            else:\n",
    "                ax[0].imshow(dec_attn_weights[0, idx].view(h, w).detach().cpu().numpy())\n",
    "                ax[0].axis('off')\n",
    "                ax[0].set_title(f'query id: {idx.item()}')\n",
    "                #ax = ax_i[1]\n",
    "                ax[1].imshow(img)\n",
    "                ax[1].add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                           fill=False, color='blue', linewidth=3))\n",
    "                ax[1].axis('off')\n",
    "                ax[1].set_title(CLASSES[probas[idx].argmax()])\n",
    "                i += 1\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        # Use the built-in COCO visualizer\n",
    "        output = postprocessors['bbox'](outputs, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
    "\n",
    "        threshold = 0.5 # set a thershold\n",
    "\n",
    "        scores = output['scores']\n",
    "        labels = output['labels']\n",
    "        boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
    "        select_mask = scores > threshold\n",
    "        box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
    "        pred_dict = {\n",
    "            'boxes': boxes[select_mask],\n",
    "            'size': targets['size'],\n",
    "            'box_label': box_label\n",
    "        }\n",
    "        vslzr = COCOVisualizer()\n",
    "        vslzr.visualize(image, pred_dict, savedir=None, caption=caption)\n",
    " \n",
    "        # Now plot the decoder's attention weights\n",
    "        total_weights = np.zeros(dec_attn_weights[0, 0].view(h, w).detach().cpu().numpy().shape)\n",
    "        for idx, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), bboxes_scaled.detach().cpu().numpy()):\n",
    "            total_weights += dec_attn_weights[0, idx].view(h, w).detach().cpu().numpy()\n",
    "            #ax[1].add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='blue', linewidth=3))\n",
    "\n",
    "        plt.imshow(total_weights)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Decoder Attention Weights (\" + caption + \")\")\n",
    "        plt.show()\n",
    "\n",
    "def visualize_encoder_weights(model, img, idxs):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "    ]\n",
    "    COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "    # propagate through the model\n",
    "    outputs = model(img.unsqueeze(0).cuda())\n",
    "\n",
    "    # keep only predictions with 0.7+ confidence\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > 0.3\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    print(\"Size is\", img.shape)\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], (img.shape[2], img.shape[1]))\n",
    "    \n",
    "    # use lists to store the outputs via up-values\n",
    "    conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
    "\n",
    "    hooks = [\n",
    "        model.backbone[-2].register_forward_hook(\n",
    "            lambda self, inp, output: conv_features.append(output)\n",
    "        ),\n",
    "        model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "            lambda self, inp, output: enc_attn_weights.append(output[1])\n",
    "        ),\n",
    "        model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "            lambda self, inp, output: dec_attn_weights.append(output[1])\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # propagate through the model\n",
    "    outputs = model(img.unsqueeze(0).cuda())\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # don't need the list anymore\n",
    "    conv_features = conv_features[0]\n",
    "    enc_attn_weights = enc_attn_weights[0]\n",
    "    dec_attn_weights = dec_attn_weights[0]\n",
    "    \n",
    "    f_map = conv_features['0']\n",
    "    print(\"Encoder attention:      \", enc_attn_weights[0].shape)\n",
    "    print(\"Feature map:            \", f_map.tensors.shape)\n",
    "    \n",
    "    # get the HxW shape of the feature maps of the CNN\n",
    "    shape = f_map.tensors.shape[-2:]\n",
    "    # and reshape the self-attention to a more interpretable shape\n",
    "    sattn = enc_attn_weights[0].reshape(shape + shape)\n",
    "    print(\"Reshaped self-attention:\", sattn.shape)\n",
    "    \n",
    "    # downsampling factor for the CNN, is 32 for DETR and 16 for DETR DC5\n",
    "    fact = 32\n",
    "    \n",
    "    img = img.permute(1, 2, 0)\n",
    "    img = (img - torch.min(img)) / (torch.max(img) - torch.min(img))\n",
    "\n",
    "    # here we create the canvas\n",
    "    plt.clf()\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(25 * 0.7, 8.5 * 0.7))\n",
    "    # and we add one plot per reference point\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    axs = [\n",
    "        fig.add_subplot(gs[0, 0]),\n",
    "        fig.add_subplot(gs[1, 0]),\n",
    "        fig.add_subplot(gs[0, -1]),\n",
    "        fig.add_subplot(gs[1, -1]),\n",
    "    ]\n",
    "\n",
    "    # for each one of the reference points, let's plot the self-attention\n",
    "    # for that point\n",
    "    for idx_o, ax in zip(idxs, axs):\n",
    "        idx = (idx_o[0] // fact, idx_o[1] // fact)\n",
    "        ax.imshow(sattn[..., idx[0], idx[1]].detach().cpu().numpy(), cmap='cividis', interpolation='nearest')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'self-attention{idx_o}')\n",
    "\n",
    "    # and now let's add the central image, with the reference points as red circles\n",
    "    fcenter_ax = fig.add_subplot(gs[:, 1:-1])\n",
    "    fcenter_ax.imshow(img.detach().cpu().numpy())\n",
    "    for (y, x) in idxs:\n",
    "        scale = img.shape[0] / img.shape[-2]\n",
    "        x = ((x // fact) + 0.5) * fact\n",
    "        y = ((y // fact) + 0.5) * fact\n",
    "        fcenter_ax.add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n",
    "        fcenter_ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_attack(model, image, targets, attack, mode, n_iter=10):\n",
    "    x_adv, etas, eta_grads, map_grads, maps = attack(model, image.unsqueeze(0).float(), vis=True, n_iter = n_iter, mode=\"vanishing\")\n",
    "    \n",
    "    # normalize grads to make them easier to see\n",
    "    perts = np.multiply(etas, maps)\n",
    "    perts_norm = [(grad - np.min(grad)) / (np.max(grad) - np.min(grad)) for grad in perts]\n",
    "    perts_norm = [np.clip(1 - np.abs(pert - np.median(pert)), 0, 1) for pert in perts_norm]\n",
    "    \n",
    "    eta_grads_norm = [(grad - np.min(grad)) / (np.max(grad)-np.min(grad)) for grad in eta_grads]\n",
    "    eta_grads_norm = [np.clip(1 - 5*np.abs(grad - np.median(grad)), 0, 1) for grad in eta_grads_norm]\n",
    "    \n",
    "    map_grads_norm = [(grad - np.min(grad)) / (np.max(grad)-np.min(grad)) for grad in map_grads]\n",
    "    map_grads_norm = [np.clip(1 - 5*np.abs(grad - np.median(grad)), 0, 1) for grad in map_grads_norm]\n",
    "    \n",
    "    etas_norm = [(eta - np.min(eta)) / (np.max(eta)-np.min(eta)) for eta in etas]\n",
    "    etas_norm = [np.clip(1 - np.abs(eta - np.median(eta)), 0, 1) for eta in etas_norm]\n",
    "    \n",
    "    maps_norm = [(m - np.min(m)) / (np.max(m)-np.min(m)) for m in maps]\n",
    "    maps_norm = [np.clip(1-np.abs(m - np.median(m)), 0.7, 1) for m in maps_norm]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=n_iter, ncols=5, figsize=(15, n_iter*3+8))\n",
    "\n",
    "    for i in range(n_iter):\n",
    "#         if i == n_iter - 1:\n",
    "#             ax[i][0].matshow(etas[i].squeeze()[:, :, 0])\n",
    "#             ax[i][1].matshow(np.zeros(etas_norm[i].squeeze()[:, :, 0].shape))\n",
    "#             ax[i][2].matshow(np.zeros(etas_norm[i].squeeze()[:, :, 0].shape))\n",
    "#             ax[i][3].hist(grads[i].flatten(), bins=100)\n",
    "#         else:\n",
    "        #ax[i].matshow(etas[i].squeeze()[:, :, 0])\n",
    "    \n",
    "        colormap = \"RdYlBu\"\n",
    "        ax[i][0].matshow(perts_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "        ax[i][1].matshow(eta_grads_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "        ax[i][2].matshow(maps_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "        ax[i][3].matshow(map_grads_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "        ax[i][4].matshow(etas_norm[i].squeeze()[0, :, :], cmap=colormap)\n",
    "#             ax[i].set_title(str(i))\n",
    "#             ax[i,:].set_axis_off()\n",
    "        ax[i][0].set_title(\"After Iter \" + str(i+1) + \": A*P.\")\n",
    "        ax[i][1].set_title(\"P Grad\")\n",
    "        ax[i][2].set_title(\"A Map\")\n",
    "        ax[i][3].set_title(\"A Grad\")\n",
    "        ax[i][4].set_title(\"P\")\n",
    "    plt.show()\n",
    "    \n",
    "# Visualize model predictions\n",
    "def viz_preds(model, image, targets, caption):\n",
    "    output = model(image[None].cuda())\n",
    "    output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
    "\n",
    "    thershold = 0.5 # set a thershold\n",
    "    scores = output['scores']\n",
    "    labels = output['labels']\n",
    "    boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
    "    select_mask = scores > thershold\n",
    "    box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
    "    pred_dict = {\n",
    "        'boxes': boxes[select_mask],\n",
    "        'size': targets['size'],\n",
    "        'box_label': box_label\n",
    "    }\n",
    "    vslzr = COCOVisualizer()\n",
    "    vslzr.visualize(image, pred_dict, savedir=None, caption=caption)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d091777",
   "metadata": {},
   "source": [
    "Select which version of DETR to use, either ResNet-50 or ResNet-101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DETR-R50 ---\n",
    "model_config_path = \"../utils/detr_utils/config/r50_config.py\"\n",
    "model_checkpoint_path = \"../model_files/detr-r50.pth\"\n",
    "args = SLConfig.fromfile(model_config_path) \n",
    "args.resume=\"resnet50\"\n",
    "args.backbone=\"resnet50\"\n",
    "\n",
    "# --- DETR-R101 ---\n",
    "# model_config_path = \"../utils/detr_utils/config/r50_config.py\"\n",
    "# model_checkpoint_path = \"../model_files/detr-r101.pth\"\n",
    "# args = SLConfig.fromfile(model_config_path) \n",
    "# args.resume=\"resnet101\"\n",
    "# args.backbone=\"resnet101\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb0678",
   "metadata": {},
   "source": [
    "Prepare model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = 'cuda'\n",
    "args.masks = False\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location='cpu', weights_only=True)\n",
    "_ = model.eval()\n",
    "model.postprocessors = postprocessors\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.to(torch.device('cuda'))\n",
    "model.criterion = criterion\n",
    "# load coco names\n",
    "with open('../utils/dino_utils/coco_id2name.json') as f:\n",
    "    id2name = json.load(f)\n",
    "    id2name = {int(k):v for k,v in id2name.items()}\n",
    "    \n",
    "args.dataset_file = 'coco'\n",
    "args.coco_path = \"../datasets/coco/\" # the path of coco\n",
    "args.fix_size = False\n",
    "\n",
    "dataset_val = build_dataset(image_set='val', args=args)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a535e0",
   "metadata": {},
   "source": [
    "Display the benign image, benign prediction, DETR's encoder's last layer self-attention weights, and DETR decoder's last layer's weights at the points in idxs. Maximum of four points supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_NUM=500 #which image to attack, up to 4999\n",
    "idxs = [(380, 1600), (400, 300),] #points in the image to investigate self-attention\n",
    "\n",
    "image, targets = dataset_val[IM_NUM]\n",
    "print(\"--- Image: \", targets[\"image_id\"][0].item(), \"---\")\n",
    "\n",
    "visualize_decoder_weights(model, image, mode=\"single\", caption=\"\")\n",
    "visualize_encoder_weights(model, image, idxs)\n",
    "viz_preds(model, image, targets, caption=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222f69b",
   "metadata": {},
   "source": [
    "Attack the image with AFOG and display the adversarial prediction as well as DETR's encoder's last layer self-attention weights at the same points. Attack mode can be changed by setting mode to either \"baseline\", \"vanishing\", or \"fabrication.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898bf34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from afog.attacks import afog\n",
    "\n",
    "# Attack the image\n",
    "image_afog = afog(model, image.unsqueeze(0).float(), n_iter=10, mode=\"baseline\")\n",
    "\n",
    "viz_preds(model, image_afog.squeeze().float(), targets, caption=\"\")\n",
    "visualize_decoder_weights(model, image_afog.squeeze().float(), mode=\"singer\", caption=\"\")\n",
    "visualize_encoder_weights(model, image_afog.squeeze().float(), idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819323a",
   "metadata": {},
   "source": [
    "Visualize the internals of AFOG as it iterates, including perturbation map, attention map, and their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a45698",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attack(model, image, targets, afog, \"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab741ba4",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d940f",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72265dc6",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AFOG",
   "language": "python",
   "name": "afog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
