---------------------------------------
Begin Slurm Prolog: Nov-05-2024 10:26:00
Job ID:    912078
User ID:   zyahn3
Account:   scs
Job name:  TOG_Plus_DINO
Partition: coc-gpu
---------------------------------------
Loading config file from dino_utils/config/DINO/DINO_4scale_swin.py
[11/05 10:26:07.983]: git:
  sha: 3382b163e7cea37d2c643d3a81a9326b6a64430d, status: has uncommited changes, branch: main

[11/05 10:26:07.983]: Command: attack_dino.py --output_dir logs/DINO/R50-MS4-%j -c dino_utils/config/DINO/DINO_4scale_swin.py --num_workers 8 --coco_path datasets/mini_coco/coco --eval --resume model_files/checkpoint0011_4scale_swin.pth --attack attention --attack_mode untargeted --sample_rate 0.001 --load_dir datasets/blackbox/focalnet_vanishing/ --load_attack 0.0 --options dn_scalar=100 embed_init_tgt=TRUE dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False dn_box_noise_scale=1.0
[11/05 10:26:07.984]: Full config saved to logs/DINO/R50-MS4-%j/config_args_all.json
[11/05 10:26:07.985]: world size: 1
[11/05 10:26:07.985]: rank: 0
[11/05 10:26:07.985]: local_rank: None
[11/05 10:26:07.985]: args: Namespace(config_file='dino_utils/config/DINO/DINO_4scale_swin.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='datasets/mini_coco/coco', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4-%j', note='', device='cuda', seed=42, resume='model_files/checkpoint0011_4scale_swin.pth', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=True, num_workers=8, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=None, amp=False, attack='attention', attack_mode='untargeted', sample_rate=0.001, load_dir='datasets/blackbox/focalnet_vanishing/', load_attack='0.0', data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='swin_L_384_22k', use_checkpoint=True, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)

/home/hice1/zyahn3/.conda/envs/TOG_test/lib/python3.12/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789220573/work/aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(config_file='dino_utils/config/DINO/DINO_4scale_swin.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='datasets/mini_coco/coco', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4-%j', note='', device='cuda', seed=42, resume='model_files/checkpoint0011_4scale_swin.pth', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=True, num_workers=8, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=None, amp=False, attack='attention', attack_mode='untargeted', sample_rate=0.001, load_dir='datasets/blackbox/focalnet_vanishing/', load_attack='0.0', data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='swin_L_384_22k', use_checkpoint=True, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)
use_checkpoint!!!!!!!!!!!!!!!!!!!!!!!!
[11/05 10:26:10.471]: number of params:217230066
[11/05 10:26:10.474]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.class_embed.0.weight": 23296,
  "transformer.decoder.class_embed.0.bias": 91,
  "transformer.tgt_embed.weight": 230400,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 23296,
  "transformer.enc_out_class_embed.bias": 91,
  "label_enc.weight": 23552,
  "input_proj.0.0.weight": 98304,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 196608,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 393216,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 3538944,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.patch_embed.proj.weight": 9216,
  "backbone.0.patch_embed.proj.bias": 192,
  "backbone.0.patch_embed.norm.weight": 192,
  "backbone.0.patch_embed.norm.bias": 192,
  "backbone.0.layers.0.blocks.0.norm1.weight": 192,
  "backbone.0.layers.0.blocks.0.norm1.bias": 192,
  "backbone.0.layers.0.blocks.0.attn.relative_position_bias_table": 3174,
  "backbone.0.layers.0.blocks.0.attn.qkv.weight": 110592,
  "backbone.0.layers.0.blocks.0.attn.qkv.bias": 576,
  "backbone.0.layers.0.blocks.0.attn.proj.weight": 36864,
  "backbone.0.layers.0.blocks.0.attn.proj.bias": 192,
  "backbone.0.layers.0.blocks.0.norm2.weight": 192,
  "backbone.0.layers.0.blocks.0.norm2.bias": 192,
  "backbone.0.layers.0.blocks.0.mlp.fc1.weight": 147456,
  "backbone.0.layers.0.blocks.0.mlp.fc1.bias": 768,
  "backbone.0.layers.0.blocks.0.mlp.fc2.weight": 147456,
  "backbone.0.layers.0.blocks.0.mlp.fc2.bias": 192,
  "backbone.0.layers.0.blocks.1.norm1.weight": 192,
  "backbone.0.layers.0.blocks.1.norm1.bias": 192,
  "backbone.0.layers.0.blocks.1.attn.relative_position_bias_table": 3174,
  "backbone.0.layers.0.blocks.1.attn.qkv.weight": 110592,
  "backbone.0.layers.0.blocks.1.attn.qkv.bias": 576,
  "backbone.0.layers.0.blocks.1.attn.proj.weight": 36864,
  "backbone.0.layers.0.blocks.1.attn.proj.bias": 192,
  "backbone.0.layers.0.blocks.1.norm2.weight": 192,
  "backbone.0.layers.0.blocks.1.norm2.bias": 192,
  "backbone.0.layers.0.blocks.1.mlp.fc1.weight": 147456,
  "backbone.0.layers.0.blocks.1.mlp.fc1.bias": 768,
  "backbone.0.layers.0.blocks.1.mlp.fc2.weight": 147456,
  "backbone.0.layers.0.blocks.1.mlp.fc2.bias": 192,
  "backbone.0.layers.0.downsample.reduction.weight": 294912,
  "backbone.0.layers.0.downsample.norm.weight": 768,
  "backbone.0.layers.0.downsample.norm.bias": 768,
  "backbone.0.layers.1.blocks.0.norm1.weight": 384,
  "backbone.0.layers.1.blocks.0.norm1.bias": 384,
  "backbone.0.layers.1.blocks.0.attn.relative_position_bias_table": 6348,
  "backbone.0.layers.1.blocks.0.attn.qkv.weight": 442368,
  "backbone.0.layers.1.blocks.0.attn.qkv.bias": 1152,
  "backbone.0.layers.1.blocks.0.attn.proj.weight": 147456,
  "backbone.0.layers.1.blocks.0.attn.proj.bias": 384,
  "backbone.0.layers.1.blocks.0.norm2.weight": 384,
  "backbone.0.layers.1.blocks.0.norm2.bias": 384,
  "backbone.0.layers.1.blocks.0.mlp.fc1.weight": 589824,
  "backbone.0.layers.1.blocks.0.mlp.fc1.bias": 1536,
  "backbone.0.layers.1.blocks.0.mlp.fc2.weight": 589824,
  "backbone.0.layers.1.blocks.0.mlp.fc2.bias": 384,
  "backbone.0.layers.1.blocks.1.norm1.weight": 384,
  "backbone.0.layers.1.blocks.1.norm1.bias": 384,
  "backbone.0.layers.1.blocks.1.attn.relative_position_bias_table": 6348,
  "backbone.0.layers.1.blocks.1.attn.qkv.weight": 442368,
  "backbone.0.layers.1.blocks.1.attn.qkv.bias": 1152,
  "backbone.0.layers.1.blocks.1.attn.proj.weight": 147456,
  "backbone.0.layers.1.blocks.1.attn.proj.bias": 384,
  "backbone.0.layers.1.blocks.1.norm2.weight": 384,
  "backbone.0.layers.1.blocks.1.norm2.bias": 384,
  "backbone.0.layers.1.blocks.1.mlp.fc1.weight": 589824,
  "backbone.0.layers.1.blocks.1.mlp.fc1.bias": 1536,
  "backbone.0.layers.1.blocks.1.mlp.fc2.weight": 589824,
  "backbone.0.layers.1.blocks.1.mlp.fc2.bias": 384,
  "backbone.0.layers.1.downsample.reduction.weight": 1179648,
  "backbone.0.layers.1.downsample.norm.weight": 1536,
  "backbone.0.layers.1.downsample.norm.bias": 1536,
  "backbone.0.layers.2.blocks.0.norm1.weight": 768,
  "backbone.0.layers.2.blocks.0.norm1.bias": 768,
  "backbone.0.layers.2.blocks.0.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.0.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.0.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.0.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.0.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.0.norm2.weight": 768,
  "backbone.0.layers.2.blocks.0.norm2.bias": 768,
  "backbone.0.layers.2.blocks.0.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.0.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.0.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.0.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.1.norm1.weight": 768,
  "backbone.0.layers.2.blocks.1.norm1.bias": 768,
  "backbone.0.layers.2.blocks.1.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.1.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.1.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.1.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.1.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.1.norm2.weight": 768,
  "backbone.0.layers.2.blocks.1.norm2.bias": 768,
  "backbone.0.layers.2.blocks.1.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.1.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.1.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.1.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.2.norm1.weight": 768,
  "backbone.0.layers.2.blocks.2.norm1.bias": 768,
  "backbone.0.layers.2.blocks.2.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.2.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.2.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.2.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.2.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.2.norm2.weight": 768,
  "backbone.0.layers.2.blocks.2.norm2.bias": 768,
  "backbone.0.layers.2.blocks.2.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.2.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.2.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.2.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.3.norm1.weight": 768,
  "backbone.0.layers.2.blocks.3.norm1.bias": 768,
  "backbone.0.layers.2.blocks.3.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.3.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.3.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.3.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.3.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.3.norm2.weight": 768,
  "backbone.0.layers.2.blocks.3.norm2.bias": 768,
  "backbone.0.layers.2.blocks.3.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.3.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.3.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.3.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.4.norm1.weight": 768,
  "backbone.0.layers.2.blocks.4.norm1.bias": 768,
  "backbone.0.layers.2.blocks.4.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.4.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.4.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.4.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.4.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.4.norm2.weight": 768,
  "backbone.0.layers.2.blocks.4.norm2.bias": 768,
  "backbone.0.layers.2.blocks.4.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.4.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.4.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.4.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.5.norm1.weight": 768,
  "backbone.0.layers.2.blocks.5.norm1.bias": 768,
  "backbone.0.layers.2.blocks.5.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.5.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.5.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.5.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.5.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.5.norm2.weight": 768,
  "backbone.0.layers.2.blocks.5.norm2.bias": 768,
  "backbone.0.layers.2.blocks.5.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.5.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.5.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.5.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.6.norm1.weight": 768,
  "backbone.0.layers.2.blocks.6.norm1.bias": 768,
  "backbone.0.layers.2.blocks.6.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.6.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.6.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.6.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.6.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.6.norm2.weight": 768,
  "backbone.0.layers.2.blocks.6.norm2.bias": 768,
  "backbone.0.layers.2.blocks.6.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.6.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.6.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.6.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.7.norm1.weight": 768,
  "backbone.0.layers.2.blocks.7.norm1.bias": 768,
  "backbone.0.layers.2.blocks.7.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.7.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.7.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.7.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.7.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.7.norm2.weight": 768,
  "backbone.0.layers.2.blocks.7.norm2.bias": 768,
  "backbone.0.layers.2.blocks.7.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.7.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.7.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.7.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.8.norm1.weight": 768,
  "backbone.0.layers.2.blocks.8.norm1.bias": 768,
  "backbone.0.layers.2.blocks.8.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.8.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.8.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.8.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.8.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.8.norm2.weight": 768,
  "backbone.0.layers.2.blocks.8.norm2.bias": 768,
  "backbone.0.layers.2.blocks.8.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.8.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.8.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.8.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.9.norm1.weight": 768,
  "backbone.0.layers.2.blocks.9.norm1.bias": 768,
  "backbone.0.layers.2.blocks.9.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.9.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.9.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.9.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.9.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.9.norm2.weight": 768,
  "backbone.0.layers.2.blocks.9.norm2.bias": 768,
  "backbone.0.layers.2.blocks.9.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.9.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.9.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.9.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.10.norm1.weight": 768,
  "backbone.0.layers.2.blocks.10.norm1.bias": 768,
  "backbone.0.layers.2.blocks.10.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.10.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.10.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.10.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.10.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.10.norm2.weight": 768,
  "backbone.0.layers.2.blocks.10.norm2.bias": 768,
  "backbone.0.layers.2.blocks.10.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.10.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.10.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.10.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.11.norm1.weight": 768,
  "backbone.0.layers.2.blocks.11.norm1.bias": 768,
  "backbone.0.layers.2.blocks.11.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.11.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.11.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.11.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.11.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.11.norm2.weight": 768,
  "backbone.0.layers.2.blocks.11.norm2.bias": 768,
  "backbone.0.layers.2.blocks.11.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.11.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.11.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.11.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.12.norm1.weight": 768,
  "backbone.0.layers.2.blocks.12.norm1.bias": 768,
  "backbone.0.layers.2.blocks.12.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.12.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.12.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.12.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.12.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.12.norm2.weight": 768,
  "backbone.0.layers.2.blocks.12.norm2.bias": 768,
  "backbone.0.layers.2.blocks.12.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.12.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.12.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.12.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.13.norm1.weight": 768,
  "backbone.0.layers.2.blocks.13.norm1.bias": 768,
  "backbone.0.layers.2.blocks.13.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.13.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.13.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.13.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.13.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.13.norm2.weight": 768,
  "backbone.0.layers.2.blocks.13.norm2.bias": 768,
  "backbone.0.layers.2.blocks.13.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.13.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.13.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.13.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.14.norm1.weight": 768,
  "backbone.0.layers.2.blocks.14.norm1.bias": 768,
  "backbone.0.layers.2.blocks.14.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.14.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.14.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.14.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.14.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.14.norm2.weight": 768,
  "backbone.0.layers.2.blocks.14.norm2.bias": 768,
  "backbone.0.layers.2.blocks.14.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.14.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.14.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.14.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.15.norm1.weight": 768,
  "backbone.0.layers.2.blocks.15.norm1.bias": 768,
  "backbone.0.layers.2.blocks.15.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.15.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.15.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.15.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.15.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.15.norm2.weight": 768,
  "backbone.0.layers.2.blocks.15.norm2.bias": 768,
  "backbone.0.layers.2.blocks.15.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.15.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.15.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.15.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.16.norm1.weight": 768,
  "backbone.0.layers.2.blocks.16.norm1.bias": 768,
  "backbone.0.layers.2.blocks.16.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.16.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.16.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.16.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.16.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.16.norm2.weight": 768,
  "backbone.0.layers.2.blocks.16.norm2.bias": 768,
  "backbone.0.layers.2.blocks.16.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.16.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.16.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.16.mlp.fc2.bias": 768,
  "backbone.0.layers.2.blocks.17.norm1.weight": 768,
  "backbone.0.layers.2.blocks.17.norm1.bias": 768,
  "backbone.0.layers.2.blocks.17.attn.relative_position_bias_table": 12696,
  "backbone.0.layers.2.blocks.17.attn.qkv.weight": 1769472,
  "backbone.0.layers.2.blocks.17.attn.qkv.bias": 2304,
  "backbone.0.layers.2.blocks.17.attn.proj.weight": 589824,
  "backbone.0.layers.2.blocks.17.attn.proj.bias": 768,
  "backbone.0.layers.2.blocks.17.norm2.weight": 768,
  "backbone.0.layers.2.blocks.17.norm2.bias": 768,
  "backbone.0.layers.2.blocks.17.mlp.fc1.weight": 2359296,
  "backbone.0.layers.2.blocks.17.mlp.fc1.bias": 3072,
  "backbone.0.layers.2.blocks.17.mlp.fc2.weight": 2359296,
  "backbone.0.layers.2.blocks.17.mlp.fc2.bias": 768,
  "backbone.0.layers.2.downsample.reduction.weight": 4718592,
  "backbone.0.layers.2.downsample.norm.weight": 3072,
  "backbone.0.layers.2.downsample.norm.bias": 3072,
  "backbone.0.layers.3.blocks.0.norm1.weight": 1536,
  "backbone.0.layers.3.blocks.0.norm1.bias": 1536,
  "backbone.0.layers.3.blocks.0.attn.relative_position_bias_table": 25392,
  "backbone.0.layers.3.blocks.0.attn.qkv.weight": 7077888,
  "backbone.0.layers.3.blocks.0.attn.qkv.bias": 4608,
  "backbone.0.layers.3.blocks.0.attn.proj.weight": 2359296,
  "backbone.0.layers.3.blocks.0.attn.proj.bias": 1536,
  "backbone.0.layers.3.blocks.0.norm2.weight": 1536,
  "backbone.0.layers.3.blocks.0.norm2.bias": 1536,
  "backbone.0.layers.3.blocks.0.mlp.fc1.weight": 9437184,
  "backbone.0.layers.3.blocks.0.mlp.fc1.bias": 6144,
  "backbone.0.layers.3.blocks.0.mlp.fc2.weight": 9437184,
  "backbone.0.layers.3.blocks.0.mlp.fc2.bias": 1536,
  "backbone.0.layers.3.blocks.1.norm1.weight": 1536,
  "backbone.0.layers.3.blocks.1.norm1.bias": 1536,
  "backbone.0.layers.3.blocks.1.attn.relative_position_bias_table": 25392,
  "backbone.0.layers.3.blocks.1.attn.qkv.weight": 7077888,
  "backbone.0.layers.3.blocks.1.attn.qkv.bias": 4608,
  "backbone.0.layers.3.blocks.1.attn.proj.weight": 2359296,
  "backbone.0.layers.3.blocks.1.attn.proj.bias": 1536,
  "backbone.0.layers.3.blocks.1.norm2.weight": 1536,
  "backbone.0.layers.3.blocks.1.norm2.bias": 1536,
  "backbone.0.layers.3.blocks.1.mlp.fc1.weight": 9437184,
  "backbone.0.layers.3.blocks.1.mlp.fc1.bias": 6144,
  "backbone.0.layers.3.blocks.1.mlp.fc2.weight": 9437184,
  "backbone.0.layers.3.blocks.1.mlp.fc2.bias": 1536,
  "backbone.0.norm1.weight": 384,
  "backbone.0.norm1.bias": 384,
  "backbone.0.norm2.weight": 768,
  "backbone.0.norm2.bias": 768,
  "backbone.0.norm3.weight": 1536,
  "backbone.0.norm3.bias": 1536
}
/home/hice1/zyahn3/.conda/envs/TOG_test/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/storage/ice1/5/9/zyahn3/TOG_plus/attack_dino.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.resume, map_location='cpu')
loading annotations into memory...
Done (t=0.08s)
creating index...
index created!
---- Attacking with <function tog_attention at 0x15548c5daca0> ----
/home/hice1/zyahn3/.conda/envs/TOG_test/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/hice1/zyahn3/.conda/envs/TOG_test/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Test:  [   0/1000]  eta: 1:01:47    time: 3.7073  data: 3.7070  max mem: 835
Test:  [  10/1000]  eta: 0:06:07    time: 0.3716  data: 0.3709  max mem: 835
Test:  [  20/1000]  eta: 0:03:22    time: 0.0311  data: 0.0304  max mem: 835
Test:  [  30/1000]  eta: 0:02:25    time: 0.0287  data: 0.0280  max mem: 835
Test:  [  40/1000]  eta: 0:01:59    time: 0.0382  data: 0.0375  max mem: 835
Test:  [  50/1000]  eta: 0:01:39    time: 0.0345  data: 0.0338  max mem: 835
Test:  [  60/1000]  eta: 0:01:27    time: 0.0301  data: 0.0293  max mem: 835
Test:  [  70/1000]  eta: 0:01:18    time: 0.0333  data: 0.0325  max mem: 835
Test:  [  80/1000]  eta: 0:01:12    time: 0.0334  data: 0.0326  max mem: 835
Test:  [  90/1000]  eta: 0:01:06    time: 0.0322  data: 0.0314  max mem: 835
Test:  [ 100/1000]  eta: 0:01:03    time: 0.0366  data: 0.0358  max mem: 835
Test:  [ 110/1000]  eta: 0:00:58    time: 0.0347  data: 0.0339  max mem: 835
Test:  [ 120/1000]  eta: 0:00:55    time: 0.0296  data: 0.0288  max mem: 835
Test:  [ 130/1000]  eta: 0:00:52    time: 0.0303  data: 0.0295  max mem: 835
Test:  [ 140/1000]  eta: 0:00:51    time: 0.0393  data: 0.0386  max mem: 835
Test:  [ 150/1000]  eta: 0:00:49    time: 0.0383  data: 0.0375  max mem: 835
Test:  [ 160/1000]  eta: 0:00:47    time: 0.0295  data: 0.0287  max mem: 835
Test:  [ 170/1000]  eta: 0:00:45    time: 0.0309  data: 0.0301  max mem: 835
Test:  [ 180/1000]  eta: 0:00:44    time: 0.0384  data: 0.0377  max mem: 835
Test:  [ 190/1000]  eta: 0:00:42    time: 0.0379  data: 0.0372  max mem: 835
Test:  [ 200/1000]  eta: 0:00:41    time: 0.0310  data: 0.0303  max mem: 835
Test:  [ 210/1000]  eta: 0:00:40    time: 0.0304  data: 0.0297  max mem: 835
Test:  [ 220/1000]  eta: 0:00:39    time: 0.0375  data: 0.0367  max mem: 835
Test:  [ 230/1000]  eta: 0:00:38    time: 0.0380  data: 0.0373  max mem: 835
Test:  [ 240/1000]  eta: 0:00:37    time: 0.0318  data: 0.0308  max mem: 835
Test:  [ 250/1000]  eta: 0:00:36    time: 0.0310  data: 0.0300  max mem: 835
Test:  [ 260/1000]  eta: 0:00:35    time: 0.0331  data: 0.0321  max mem: 835
Test:  [ 270/1000]  eta: 0:00:34    time: 0.0334  data: 0.0322  max mem: 835
Test:  [ 280/1000]  eta: 0:00:33    time: 0.0344  data: 0.0334  max mem: 835
Test:  [ 290/1000]  eta: 0:00:32    time: 0.0351  data: 0.0342  max mem: 835
Test:  [ 300/1000]  eta: 0:00:32    time: 0.0300  data: 0.0292  max mem: 835
Test:  [ 310/1000]  eta: 0:00:31    time: 0.0294  data: 0.0287  max mem: 835
Test:  [ 320/1000]  eta: 0:00:30    time: 0.0342  data: 0.0334  max mem: 835
Test:  [ 330/1000]  eta: 0:00:29    time: 0.0348  data: 0.0339  max mem: 835
Test:  [ 340/1000]  eta: 0:00:29    time: 0.0311  data: 0.0299  max mem: 835
Test:  [ 350/1000]  eta: 0:00:28    time: 0.0324  data: 0.0314  max mem: 835
Test:  [ 360/1000]  eta: 0:00:27    time: 0.0344  data: 0.0333  max mem: 835
Test:  [ 370/1000]  eta: 0:00:27    time: 0.0322  data: 0.0311  max mem: 835
Test:  [ 380/1000]  eta: 0:00:26    time: 0.0341  data: 0.0331  max mem: 835
Test:  [ 390/1000]  eta: 0:00:26    time: 0.0347  data: 0.0330  max mem: 835
Test:  [ 400/1000]  eta: 0:00:25    time: 0.0292  data: 0.0276  max mem: 835
Test:  [ 410/1000]  eta: 0:00:24    time: 0.0298  data: 0.0290  max mem: 835
Test:  [ 420/1000]  eta: 0:00:24    time: 0.0367  data: 0.0359  max mem: 835
Test:  [ 430/1000]  eta: 0:00:23    time: 0.0356  data: 0.0346  max mem: 835
Test:  [ 440/1000]  eta: 0:00:23    time: 0.0303  data: 0.0289  max mem: 835
Test:  [ 450/1000]  eta: 0:00:22    time: 0.0296  data: 0.0277  max mem: 835
Test:  [ 460/1000]  eta: 0:00:22    time: 0.0378  data: 0.0363  max mem: 835
Test:  [ 470/1000]  eta: 0:00:21    time: 0.0388  data: 0.0378  max mem: 835
Test:  [ 480/1000]  eta: 0:00:21    time: 0.0294  data: 0.0284  max mem: 835
Test:  [ 490/1000]  eta: 0:00:20    time: 0.0319  data: 0.0311  max mem: 835
Test:  [ 500/1000]  eta: 0:00:20    time: 0.0315  data: 0.0307  max mem: 835
Test:  [ 510/1000]  eta: 0:00:19    time: 0.0304  data: 0.0296  max mem: 835
Test:  [ 520/1000]  eta: 0:00:19    time: 0.0351  data: 0.0343  max mem: 835
Test:  [ 530/1000]  eta: 0:00:19    time: 0.0407  data: 0.0398  max mem: 835
Test:  [ 540/1000]  eta: 0:00:18    time: 0.0337  data: 0.0326  max mem: 835
Test:  [ 550/1000]  eta: 0:00:17    time: 0.0284  data: 0.0275  max mem: 835
Test:  [ 560/1000]  eta: 0:00:17    time: 0.0353  data: 0.0345  max mem: 835
Test:  [ 570/1000]  eta: 0:00:17    time: 0.0423  data: 0.0413  max mem: 835
Test:  [ 580/1000]  eta: 0:00:16    time: 0.0358  data: 0.0346  max mem: 835
Test:  [ 590/1000]  eta: 0:00:16    time: 0.0304  data: 0.0294  max mem: 835
Test:  [ 600/1000]  eta: 0:00:15    time: 0.0341  data: 0.0333  max mem: 835
Test:  [ 610/1000]  eta: 0:00:15    time: 0.0362  data: 0.0354  max mem: 835
Test:  [ 620/1000]  eta: 0:00:14    time: 0.0306  data: 0.0299  max mem: 835
Test:  [ 630/1000]  eta: 0:00:14    time: 0.0334  data: 0.0326  max mem: 835
Test:  [ 640/1000]  eta: 0:00:14    time: 0.0369  data: 0.0361  max mem: 835
Test:  [ 650/1000]  eta: 0:00:13    time: 0.0332  data: 0.0324  max mem: 835
Test:  [ 660/1000]  eta: 0:00:13    time: 0.0308  data: 0.0299  max mem: 835
Test:  [ 670/1000]  eta: 0:00:12    time: 0.0356  data: 0.0348  max mem: 835
Test:  [ 680/1000]  eta: 0:00:12    time: 0.0379  data: 0.0371  max mem: 835
Test:  [ 690/1000]  eta: 0:00:12    time: 0.0301  data: 0.0294  max mem: 835
Test:  [ 700/1000]  eta: 0:00:11    time: 0.0278  data: 0.0271  max mem: 835
Test:  [ 710/1000]  eta: 0:00:11    time: 0.0336  data: 0.0325  max mem: 835
Test:  [ 720/1000]  eta: 0:00:10    time: 0.0413  data: 0.0395  max mem: 835
Test:  [ 730/1000]  eta: 0:00:10    time: 0.0342  data: 0.0326  max mem: 835
Test:  [ 740/1000]  eta: 0:00:09    time: 0.0270  data: 0.0257  max mem: 835
Test:  [ 750/1000]  eta: 0:00:09    time: 0.0291  data: 0.0280  max mem: 835
Test:  [ 760/1000]  eta: 0:00:09    time: 0.0389  data: 0.0379  max mem: 835
Test:  [ 770/1000]  eta: 0:00:08    time: 0.0372  data: 0.0362  max mem: 835
Test:  [ 780/1000]  eta: 0:00:08    time: 0.0273  data: 0.0261  max mem: 835
Test:  [ 790/1000]  eta: 0:00:07    time: 0.0270  data: 0.0260  max mem: 835
Test:  [ 800/1000]  eta: 0:00:07    time: 0.0380  data: 0.0370  max mem: 835
Test:  [ 810/1000]  eta: 0:00:07    time: 0.0388  data: 0.0378  max mem: 835
Test:  [ 820/1000]  eta: 0:00:06    time: 0.0278  data: 0.0268  max mem: 835
Test:  [ 830/1000]  eta: 0:00:06    time: 0.0279  data: 0.0267  max mem: 835
Test:  [ 840/1000]  eta: 0:00:06    time: 0.0398  data: 0.0386  max mem: 835
Test:  [ 850/1000]  eta: 0:00:05    time: 0.0386  data: 0.0377  max mem: 835
Test:  [ 860/1000]  eta: 0:00:05    time: 0.0264  data: 0.0255  max mem: 835
Test:  [ 870/1000]  eta: 0:00:04    time: 0.0279  data: 0.0269  max mem: 835
Test:  [ 880/1000]  eta: 0:00:04    time: 0.0395  data: 0.0385  max mem: 835
Test:  [ 890/1000]  eta: 0:00:04    time: 0.0384  data: 0.0377  max mem: 835
Test:  [ 900/1000]  eta: 0:00:03    time: 0.0272  data: 0.0264  max mem: 835
Test:  [ 910/1000]  eta: 0:00:03    time: 0.0277  data: 0.0265  max mem: 835
Test:  [ 920/1000]  eta: 0:00:03    time: 0.0406  data: 0.0394  max mem: 835
Mult is, 0.37878786373620144
Orig shape is torch.Size([3, 800, 1204]) Samples shape is torch.Size([1, 3, 800, 1204])
/storage/ice1/5/9/zyahn3/TOG_plus/dino_utils/engine.py:211: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
NP orig is [[[-0.7957497  -0.7957497  -0.7957497  ... -0.20546462 -0.21195127
   -0.21195127]
  [-0.7957497  -0.7957497  -0.7957497  ... -0.20546462 -0.21195127
   -0.21195127]
  [-0.7957497  -0.7957497  -0.7957497  ... -0.20546462 -0.21195127
   -0.21195127]
  ...
  [-0.7049366  -0.7049366  -0.7049366  ... -0.39357746 -0.39357746
   -0.39357746]
  [-0.69844997 -0.69844997 -0.69844997 ... -0.4000641  -0.39357746
   -0.39357746]
  [-0.69844997 -0.69844997 -0.69844997 ... -0.4000641  -0.39357746
   -0.39357746]]

 [[-0.7644725  -0.7644725  -0.7644725  ... -0.18753712 -0.19416857
   -0.19416857]
  [-0.7644725  -0.7644725  -0.7644725  ... -0.18753712 -0.19416857
   -0.19416857]
  [-0.7644725  -0.7644725  -0.7644725  ... -0.18753712 -0.19416857
   -0.19416857]
  ...
  [-0.61858076 -0.61858076 -0.61858076 ... -0.5721607  -0.5787921
   -0.5787921 ]
  [-0.6119493  -0.6119493  -0.6119493  ... -0.5787921  -0.5721607
   -0.5721607 ]
  [-0.6119493  -0.6119493  -0.6119493  ... -0.5787921  -0.5721607
   -0.5721607 ]]

 [[-0.6768997  -0.6768997  -0.6768997  ... -0.10913048 -0.11573245
   -0.11573245]
  [-0.6768997  -0.6768997  -0.6768997  ... -0.10913048 -0.11573245
   -0.11573245]
  [-0.6768997  -0.6768997  -0.6768997  ... -0.10913048 -0.11573245
   -0.11573245]
  ...
  [-0.406219   -0.406219   -0.406219   ... -0.54486036 -0.55146235
   -0.55146235]
  [-0.39961708 -0.39961708 -0.39961708 ... -0.55146235 -0.54486036
   -0.54486036]
  [-0.39961708 -0.39961708 -0.39961708 ... -0.55146235 -0.54486036
   -0.54486036]]] NP sample is [[[-0.78795148 -0.78337029 -0.78123885 ... -0.23683716 -0.21060166
   -0.22301561]
  [-0.78088612 -0.77878914 -0.7685046  ... -0.23598995 -0.20477332
   -0.22719185]
  [-0.76512343 -0.80223631 -0.77219646 ... -0.23683716 -0.2075627
   -0.22836272]
  ...
  [-0.67387369 -0.72581635 -0.67356406 ... -0.40637902 -0.41642608
   -0.3869083 ]
  [-0.70062517 -0.7298225  -0.69059777 ... -0.41556339 -0.37477503
   -0.36905678]
  [-0.71401734 -0.71406884 -0.69502087 ... -0.39706143 -0.38084907
   -0.38894137]]

 [[-0.77028149 -0.78635469 -0.74095051 ... -0.16804317 -0.1628867
   -0.17000515]
  [-0.73977918 -0.79584499 -0.77271834 ... -0.21890966 -0.16279601
   -0.21003036]
  [-0.79055462 -0.75680111 -0.78811375 ... -0.19529041 -0.20727149
   -0.17095311]
  ...
  [-0.64990118 -0.60332935 -0.59501889 ... -0.60353323 -0.58592171
   -0.57936228]
  [-0.60014864 -0.63558758 -0.58834387 ... -0.60259632 -0.60353323
   -0.55854781]
  [-0.62741859 -0.6281982  -0.58057675 ... -0.61016465 -0.56232397
   -0.54838229]]

 [[-0.64552712 -0.66911342 -0.65294516 ... -0.13271914 -0.13908893
   -0.13975312]
  [-0.65341999 -0.70735449 -0.6898546  ... -0.07931809 -0.09875785
   -0.08466211]
  [-0.65703408 -0.69150796 -0.65334692 ... -0.10889533 -0.13983806
   -0.08517227]
  ...
  [-0.39992271 -0.38850289 -0.38433747 ... -0.53715415 -0.56692759
   -0.52008978]
  [-0.42317353 -0.3773369  -0.39100829 ... -0.57516797 -0.5134878
   -0.55040746]
  [-0.42316412 -0.40446855 -0.36824452 ... -0.52008978 -0.52494798
   -0.5134878 ]]]
Test:  [ 930/1000]  eta: 0:00:03  class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)  time: 0.4711  data: 0.0313  max mem: 9348
Test:  [ 940/1000]  eta: 0:00:02  class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)  time: 0.4562  data: 0.0140  max mem: 9348
Test:  [ 950/1000]  eta: 0:00:02  class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)  time: 0.0236  data: 0.0176  max mem: 9348
Test:  [ 960/1000]  eta: 0:00:01  class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)  time: 0.0283  data: 0.0246  max mem: 9348
Test:  [ 970/1000]  eta: 0:00:01  class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)  time: 0.0253  data: 0.0241  max mem: 9348
Test:  [ 980/1000]  eta: 0:00:00  class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)  time: 0.0237  data: 0.0225  max mem: 9348
Test:  [ 990/1000]  eta: 0:00:00  class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)  time: 0.0252  data: 0.0242  max mem: 9348
Test:  [ 999/1000]  eta: 0:00:00  class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)  time: 0.0139  data: 0.0131  max mem: 9348
Test: Total time: 0:00:45 (0.0455 s / it)


------- Similarity and Timing Metrics -------
Average Attack Time: 8.2399
Average L2 Norm: 0.0012
Average L0 Norm: 0.9730
Average SSIM 0.8251
Mean Difference in Images: 0.0173
Median Difference in Images: 0.0160
Max Difference in Images: 0.0314
------------------------------------------------


Averaged stats: class_error: 100.00  loss: 82.3468 (82.3468)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 14.9708 (14.9708)  loss_bbox: 0.4547 (0.4547)  loss_giou: 0.2351 (0.2351)  loss_ce_0: 7.5372 (7.5372)  loss_bbox_0: 0.4562 (0.4562)  loss_giou_0: 0.2238 (0.2238)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 9.2669 (9.2669)  loss_bbox_1: 0.5358 (0.5358)  loss_giou_1: 0.2545 (0.2545)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 10.5423 (10.5423)  loss_bbox_2: 0.5379 (0.5379)  loss_giou_2: 0.2541 (0.2541)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 12.5396 (12.5396)  loss_bbox_3: 0.5393 (0.5393)  loss_giou_3: 0.2553 (0.2553)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 13.7966 (13.7966)  loss_bbox_4: 0.4561 (0.4561)  loss_giou_4: 0.2356 (0.2356)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 8.4307 (8.4307)  loss_bbox_interm: 0.5389 (0.5389)  loss_giou_interm: 0.2853 (0.2853)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 14.9708 (14.9708)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0909 (0.0909)  loss_giou_unscaled: 0.1176 (0.1176)  loss_xy_unscaled: 0.0306 (0.0306)  loss_hw_unscaled: 0.0603 (0.0603)  cardinality_error_unscaled: 899.0000 (899.0000)  loss_ce_0_unscaled: 7.5372 (7.5372)  loss_bbox_0_unscaled: 0.0912 (0.0912)  loss_giou_0_unscaled: 0.1119 (0.1119)  loss_xy_0_unscaled: 0.0285 (0.0285)  loss_hw_0_unscaled: 0.0628 (0.0628)  cardinality_error_0_unscaled: 899.0000 (899.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 9.2669 (9.2669)  loss_bbox_1_unscaled: 0.1072 (0.1072)  loss_giou_1_unscaled: 0.1273 (0.1273)  loss_xy_1_unscaled: 0.0355 (0.0355)  loss_hw_1_unscaled: 0.0717 (0.0717)  cardinality_error_1_unscaled: 899.0000 (899.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 10.5423 (10.5423)  loss_bbox_2_unscaled: 0.1076 (0.1076)  loss_giou_2_unscaled: 0.1271 (0.1271)  loss_xy_2_unscaled: 0.0358 (0.0358)  loss_hw_2_unscaled: 0.0717 (0.0717)  cardinality_error_2_unscaled: 899.0000 (899.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 12.5396 (12.5396)  loss_bbox_3_unscaled: 0.1079 (0.1079)  loss_giou_3_unscaled: 0.1276 (0.1276)  loss_xy_3_unscaled: 0.0363 (0.0363)  loss_hw_3_unscaled: 0.0716 (0.0716)  cardinality_error_3_unscaled: 899.0000 (899.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 13.7966 (13.7966)  loss_bbox_4_unscaled: 0.0912 (0.0912)  loss_giou_4_unscaled: 0.1178 (0.1178)  loss_xy_4_unscaled: 0.0309 (0.0309)  loss_hw_4_unscaled: 0.0604 (0.0604)  cardinality_error_4_unscaled: 899.0000 (899.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 8.4307 (8.4307)  loss_bbox_interm_unscaled: 0.1078 (0.1078)  loss_giou_interm_unscaled: 0.1427 (0.1427)  loss_xy_interm_unscaled: 0.0418 (0.0418)  loss_hw_interm_unscaled: 0.0660 (0.0660)  cardinality_error_interm_unscaled: 899.0000 (899.0000)
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
---------------------------------------
Begin Slurm Epilog: Nov-05-2024 10:26:58
Job ID:        912078
Array Job ID:  _4294967294
User ID:       zyahn3
Account:       scs
Job name:      TOG_Plus_DINO
Resources:     cpu=1,gres/gpu:a100=1,mem=64G,node=1
Rsrc Used:     cput=00:00:58,vmem=0,walltime=00:00:58,mem=6616K,energy_used=0
Partition:     coc-gpu
Nodes:         atl1-1-01-005-15-0
---------------------------------------
